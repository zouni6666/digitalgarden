---
{"dg-publish":true,"permalink":"/02 本科毕设/benchmark/0 精细映射与位点定义/"}
---


## 代码

```shell
cd /home/data6/zhanghao/pathway/pathway/pigGWAS/finemap/AG      #使用17服务器
```
从第一个文件开始：
```shell
sum_file=AG.fastGWA
trait=AG
ln -s ../../$sum_file .
```
ca gwas：
1. 提取显著位点区间

```shell
##先粗略查看p值分布情况 
cat $sum_file |cut -f10|sort -k1g|uniq| cat -n|less > abc.txt


#阈值取5e-6，取从小到大排列前100个，如果数量不够就取5e-6的
cat $sum_file | awk 'BEGIN{OFS=FS="\t"} $10<5e-6' | sort -k10,10g | head -n 100 > q
cat q|awk 'BEGIN{OFS=FS="\t"}{print $1,$3-1500000,$3+1500000}' |awk 'BEGIN{OFS=FS="\t"}{print $1,$3-1500000,$3+1500000}' >q2 
#按位置排序
sort -k1,1 -k2,2n q2 > q2_sorted 
bedtools merge -i q2_sorted > region.txt 
#rm q q2_sorted 
#bedtools intersect -a region.txt -b q4 -wa -wb > region_snp.txt
```
2. 取交集
```shell
#得到猪的panel文件 /home/data6/zhanghao/pathway/pathway/pigGWAS/panel/chr
#对18个vcf文件建立索引
tabix -p vcf home/data6/zhanghao/pathway/pathway/pigGWAS/panel/chr/chr*.vcf.gz 
#使用bcftools合并
bcftools concat home/data6/zhanghao/pathway/pathway/pigGWAS/panel/chr/chr*.vcf.gz -o /home/data6/zhanghao/pathway/pathway/pigGWAS/panel/all.vcf.gz -O z
cat /home/data6/zhanghao/pathway/pathway/pigGWAS/panel/pig.bim | cut -f 2 > /home/data6/zhanghao/pathway/pathway/pigGWAS/panel/snp_panel.txt
sort /home/data6/zhanghao/pathway/pathway/pigGWAS/panel/snp_panel.txt > /home/data6/zhanghao/pathway/pathway/pigGWAS/panel/sorted_snp_panel.txt 


cat $sum_file |cut -f2 |tail -n+2 >snp.txt 
sort snp.txt > sorted_snp.txt 
comm -12 /home/data6/zhanghao/pathway/pathway/pigGWAS/panel/sorted_snp_panel.txt sorted_snp.txt > all_common_snp.txt
```
plink 根据 snp 名提取位点信息 
```shell
plink --bfile /home/data6/zhanghao/pathway/pathway/pigGWAS/panel/pig --extract all_common_snp.txt --make-bed --out all_common_snp_sum --keep-allele-order --threads 60
cut -f2 all_common_snp_sum.bim >all_common_snp_sum.order 
cat $sum_file| grep --file all_common_snp_sum.order -w > all_common_snp_sum_fi.txt
```
3. 逐个区域提取数据
```shell
while FS=OFS='\t' read -r col1 col2 col3; do
echo "process region_${col1}_${col2}_${col3}"
echo -e "${col1}\t${col2}\t${col3}\ta" > sig_loc_bed.txt
a=region_${col1}_${col2}_${col3}
plink --bfile all_common_snp_sum --extract range sig_loc_bed.txt --make-bed --out plink_$a --keep-allele-order --threads 60
plink --bfile plink_$a --r square --out plink_$a --threads 60 &
cut -f2 plink_${a}.bim > region_snp_${col1}_${col2}_${col3}.txt
cat all_common_snp_sum_fi.txt | grep --file region_snp_${col1}_${col2}_${col3}.txt -w > sum_${a}.txt
done < region.txt
```
4. 获取精细映射位点信息
```shell
while FS=OFS='\t' read -r col1 col2 col3; do
echo "process region_${col1}_${col2}_${col3}"
echo -e "${col1}\t${col2}\t${col3}\ta" > sig_loc_bed.txt
a=region_${col1}_${col2}_${col3}
# 获取精细映射位点信息 
/home/data6/zhanghao/miniconda/envs/R/bin/Rscript  /home/data6/zhanghao/pathway/pathway/fimemapping_susie.R \
all_common_snp_sum_fi.txt sum_${a}.txt plink_$a.ld \
517 ${trait} ./ &    #517是样本数  trait是一个标记
done < region.txt
```
得到的重要文件有：
- 所有区间 `all_common_snp.txt，all_common_snp_sum_fi.txt， region.txt， plink_all_common_snp_sum*`
- 区间特异 `plink_$a，plink_$a.ld, region_snp_*.txt. sum_*.tx`

根据区间提取 snp：
```shell
# plink --bfile extracted_data1 --extract range sig_loc_bed.txt --make-bed --out fine_loucs --keep-allele-order
```
## finemapping 
[https://www.cnblogs.com/shiyanhe/p/13271838.html](https://www.cnblogs.com/shiyanhe/p/13271838.html)
官网介绍 [https://www.christianbenner.com/](https://www.christianbenner.com/)
根据提供的信息，研究使用了 SuSiE 方法对英国生物库（UK Biobank）中的95个复杂性状和 GTEx v.8中的49个组织进行了21个精细映射。每个区域允许最多有10个因果变异体(L=10)。先验方差和残差方差使用默认选项进行估计，并使用标准纯度过滤器对单个效应（潜在的95%可信集）进行了修剪，以确保任何一对可信集中的变异体的 r<sup>2</sup> 值不大于0.25。对于每个特征，将最显著相关的变异体为中心，在其周围±1.5 Mb 的区域定义为一个区域，并合并重叠的区域。作为 SuSiE 的输入，使用 BOLT-LMM 对定量性状和 SAIGE 对二元性状进行了每个区域的汇总统计，样本剂量的 LD 使用 LDStore 进行计算，并通过经验方法计算表型方差。排除了主要组织相容性复合物区域（chr6: 25-36 Mb）以及包含<100个最小等位基因数（MACs）的95%可信集。对编码区（非义变异和预测的功能丧失变异）进行了 Variant Effect Predictor v.85（ref. 62）的注释。本研究使用的精细映射数据可在 https://www.finucanelab.org/data 上获得。
>[!note]+ 原文
>Fine-mapping was performed for 95 complex traits in the UK Biobank and 49 tissues in GTEx v.8 using the Sum of Single Effects (SuSiE) method, allowing for up to 10 causal variants in each region. Prior variance and residual variance were estimated using the default options and single effects (potential 95% CSs) were pruned using the standard purity filter such that no pair of variants in a CS could have r 2 > 0.25. Regions were defined for each trait as ±1.5 Mb around the most significantly associated variant and overlapping regions were merged. As inputs to SuSiE, summary statistics for each region were obtained using BOLT-LMM 50 for quantitative traits and SAIGE 51 for binary traits, in sample dosage LD was computed using LDStore and phenotypic variance was computed empirically. Variants in the major histocompatibility complex region (chr 6: 25–36 Mb) were excluded, as were 95% CSs containing variants with <100 MACs. Coding (missense and predicted loss of function) variants were annotated using the Variant Effect Predictor v.85.

### 解释
这段文字描述的是使用 SuSiE 方法对 UK Biobank 中 95 个复杂性状和 GTEx v.8 中 49 个组织的基因组数据进行精细定位研究的过程。SuSiE 方法允许每个区域内最多包含 10 个因果变异位点。在此过程中，为了确定哪些变异位点可能与特定性状相关，研究人员首先定义了每个性状的关注区域，这些区域是围绕最显著关联的变异位点的±1.5 Mb 范围内。然后，利用 BOLT-LMM 和 SAIGE 工具从这些区域中获取总结统计数据，分别用于定量性状和二元性状的分析。接着，使用 LDStore 计算样本剂量的连锁不平衡（LD），并且实证地计算表型方差。此外，排除了主要组织相容性复合体区域（即 MHC 区域）内的变异位点，以及少于 100 个最小等位基因计数（MACs）的 95%置信区间（CSs）内的变异位点。编码变异（错义变异和预测的功能丧失变异）使用 Variant Effect Predictor v.85进行注释。
**简化解释：**
想象我们在寻找哪些遗传变异与人类的复杂疾病或性状（如身高、体重、疾病易感性等）有关。这个过程就像在一个庞大的迷宫中寻找关键的线索。UK Biobank 和 GTEx v.8 是两个大型的生物信息库，它们收集了大量的遗传和健康信息。我们使用 SuSiE 方法作为一种先进的工具，来识别可能与特定性状相关的关键遗传变异位点。
首先，我们定义了“关注区域”，这些区域是围绕我们认为可能与性状有关的遗传变异位点的特定范围。我们使用特定的统计工具（如 BOLT-LMM 和 SAIGE）从这些区域中提取信息，这些信息有助于我们理解这些变异位点如何与性状关联。
然后，我们通过计算变异位点之间的关联程度（连锁不平衡），以及实际观察到的性状差异（表型方差），来进一步缩小可能与性状有关的变异位点的范围。
在这一过程中，我们排除了一些可能干扰结果的变异位点，例如那些在主要组织相容性复合体（MHC）区域内的变异位点，因为这个区域内的变异位点特别多，且它们之间的关联复杂，容易引起分析上的混淆。同时，也排除了那些存在量不足的变异位点，以确保结果的可靠性。
最后，对于我们认为与性状有关的变异位点，我们还会进一步检查它们是否会改变蛋白质的结构或功能（例如，错义变异或功能丧失变异），因为这样的变异更有可能对性状产生直接影响。
简而言之，我们通过这一系列复杂但精确的分析步骤，试图揭示遗传变异如何影响人类的复杂性状，从而增进我们对遗传与疾病之间关系的理解。
### 代码
```shell
cd /home/fenglijun/data3/project/data/human/gtex/hg19/v7/eQTL/test
zcat 90037.v1.1.fastGWA.gz |cut -f2 |tail -n+2 >snp.txt
l g1000_eur.bim |cut -f2 >snp_panel.txt

sort snp_panel.txt > sorted_snp_panel.txt
sort snp.txt > sorted_snp.txt
comm -12 sorted_snp_panel.txt sorted_snp.txt >common_snp.txt

# plink 根据snp名提取位点信息
plink --bfile g1000_eur --extract common_snp.txt --make-bed --out extracted_data1 --keep-allele-order

# 提取显著位点区间
l 90037.v1.1.fastGWA|awk 'BEGIN{OFS=FS="\t"}{if($10<0.000001) print $0}' >w
l w|cut -f 1,3|awk 'GEGIN{OFS=FS="\t"}{print $1,$2-1500000,$2+1500000,$1"_"$2}' >sig_loc_bed.txt

# 根据区间提取snp
plink --bfile  extracted_data1   --extract range  sig_loc_bed.txt  --make-bed --out fine_loucs --keep-allele-order

# 用 plink 的 --r square 计算出 r matrix，
plink --bfile fine_loucs  --r square  'spaces' --out fine_loucs_r2 

# 或许finemao所需摘要格式
cat fine_loucs.bim |cut -f2 >fine_snp.txt
grep --file fine_snp.txt  -w 90037.v1.1.fastGWA |awk 'BEGIN{OFS=FS="\t"}{print  $2,$1,$3,$5,$4,1-$7,$8,$9}' |awk 'BEGIN{OFS=FS="\t"}{if($6>0.5) print  $1,$2,$3,$4,$5,1-$6,$7,$8; else print $0}'> fine_locs.txt

# 是一个能够识别causal SNPs，能够估计causal SNPs效应大小，以及估计causal SNPs遗传力分布的程序。
# FINEMAP 的优点是，在设定好最大causal SNPs 数量 k 后，得到的结果包含了 1 到 k 个不同 causal SNPs 的后验概率以及各个 causal SNPs 数量的概率，非常方便进行后续的分析。
finemap_v1.4.2_x86_64 --sss --in-files mast --dataset 1 --n-causal-snps 10 --n-threads 40

# mast文件
# ld 列和 bcor 列二选一，如果 ld 信息是用纯文本表示的矩阵，填入 ld 文件即可。
z;ld;snp;config;cred;log;n_samples
dataset1.z;dataset1.ld;dataset1.snp;dataset1.config;dataset1.cred;dataset1.log;5363
dataset2.z;dataset2.ld;dataset2.snp;dataset2.config;dataset2.cred;dataset2.log;5363
# z：Z 文件的名称（输入）
# ld：LD 文件的名称（输入）
# bcor：BCOR 文件的名称（输入）
# snp：结果输出 SNP 文件的名称（输出）
# config：结果输出 CONFIG 文件的名称（输出）
# cred：结果输出 CRED 文件的名称（输出）
# n_samples：GWAS 样本数量
# k：K 文件的名称（可选输入，可忽略）
# log：LOG文件的名称（可选输出，可忽略）

#输出结果有 .snp、.cred、.config 三种不同后缀的文件。
.config 记录了分析时选用的参数。
.snp 文件是 model-averaged posterior summaries，每一行是一个 SNP。在假定不同 causal SNPs 数量时会得到不一样的结果，而这个文件包含了所有结果的摘要情况。
.cred 文件最后会带有一个数字，这个数字代表的是 causal SNPs 数量 k。比如，.cred5 是设定 causal SNPs 为5个而得到的计算结果。这个文件中，包含了数量 k 的后验概率和和推断出的 causal SNP 后验概率。

# 查询指定的SNP是否为因果SNP
finemap_v1.4.2_x86_64  --in-files mast --dataset 1  --rsids  rs62149822,rs7556454 --config
```
![02 本科毕设/benchmark/attachment/Untitled.png](/img/user/02%20%E6%9C%AC%E7%A7%91%E6%AF%95%E8%AE%BE/benchmark/attachment/Untitled.png)
![02 本科毕设/benchmark/attachment/Untitled2 1.png](/img/user/02%20%E6%9C%AC%E7%A7%91%E6%AF%95%E8%AE%BE/benchmark/attachment/Untitled2%201.png)
![02 本科毕设/benchmark/attachment/Untitled3 1.png](/img/user/02%20%E6%9C%AC%E7%A7%91%E6%AF%95%E8%AE%BE/benchmark/attachment/Untitled3%201.png)
#### 37

![02 本科毕设/benchmark/attachment/Untitled111.png](/img/user/02%20%E6%9C%AC%E7%A7%91%E6%AF%95%E8%AE%BE/benchmark/attachment/Untitled111.png)
#### 38
![02 本科毕设/benchmark/attachment/Untitled222.png](/img/user/02%20%E6%9C%AC%E7%A7%91%E6%AF%95%E8%AE%BE/benchmark/attachment/Untitled222.png)
## susie 精细映射
[Articles • susieR](https://stephenslab.github.io/susieR/articles/index.html)
```shell


sum_file=174.11_PheCode.v1.0.fastGWA.gz
ln -s ../$sum_file .
# 1 获取摘要数据 和 参考面板共有snp
cd /home/fenglijun/data3/project/data/human/gwas_summary/immune/q
ca gwas

# 1  提取显著位点区间
zcat $sum_file |awk 'BEGIN{OFS=FS="\t"}{if($13<5e-8) print $0}' >q
cat q|awk 'BEGIN{OFS=FS="\t"}{print $1,$3-1500000,$3+1500000}' |awk 'BEGIN{OFS=FS="\t"}{print $1,$3-1500000,$3+1500000}' >q2
bedtools merge -i q2 >region.txt
rm q q2
#bedtools intersect -a region.txt  -b q4 -wa -wb > region_snp.txt

# 2 g1000_eur.bim |cut -f2 >snp_panel.txt
#sort snp_panel.txt > sorted_snp_panel.txt
zcat $sum_file |cut -f2 |tail -n+2 >snp.txt
sort snp.txt > sorted_snp.txt
comm -12 /home/fenglijun/data3/project/data/human/reference/panel/sorted_snp_panel.txt  sorted_snp.txt >all_common_snp.txt

## plink 根据snp名提取位点信息
plink --bfile /home/fenglijun/data3/project/data/human/reference/panel/g1000_eur --extract all_common_snp.txt --make-bed --out  all_common_snp_sum  --keep-allele-order
cut -f2 all_common_snp_sum.bim >all_common_snp_sum.order
zcat $sum_file| grep --file all_common_snp_sum.order  -w > all_common_snp_sum_fi.txt

# 3 逐个区域提取数据
while FS=OFS='\t ' read -r col1 col2 col3; do
  echo "process region_${col1}_${col2}_${col3}"
  echo -e "${col1}\t${col2}\t${col3}\ta" > sig_loc_bed.txt
	a=region_${col1}_${col2}_${col3}

	# 1
	# plink根据区间提取snp，并计算每个区间的r
	# 用 plink 的 --r square 计算出 r matrix，
  # plink --bfile all  --r square  'spaces' --out all_r 输出用空格分隔的数据
	plink --bfile  all_common_snp_sum   --extract range  sig_loc_bed.txt  --make-bed --out plink_$a --keep-allele-order
	plink --bfile plink_$a  --r square   --out plink_$a &

	# 2 提供区间共有位点摘要数据
	cut -f2 plink_${a}.bim > region_snp_${col1}_${col2}_${col3}.txt
  cat all_common_snp_sum_fi.txt | grep --file region_snp_${col1}_${col2}_${col3}.txt  -w > sum_${a}.txt
done < region.txt

# 4 获取精细映射位点信息
while FS=OFS='\t ' read -r col1 col2 col3; do
  echo "process region_${col1}_${col2}_${col3}"
  echo -e "${col1}\t${col2}\t${col3}\ta" > sig_loc_bed.txt
	a=region_${col1}_${col2}_${col3}
	# 获取精细映射位点信息
	 Rscript /home/fenglijun/data3/project/2_gene_geneset/benchmark/10/1_finemapping/a.R all_common_snp_sum_fi.txt sum_${a}.txt plink_$a.ld \
   456348 t_10 /home/fenglijun/data3/project/2_gene_geneset/benchmark/10/1_finemapping/ &
done < region.txt

#得到的重要文件有 
#所有区间all_common_snp.txt，all_common_snp_sum_fi.txt，  region.txt， plink_all_common_snp_sum*
#区间特异 plink_$a，plink_$a.ld, region_snp_*.txt. sum_*.tx

### 根据区间提取snp
##  plink --bfile  extracted_data1   --extract range  sig_loc_bed.txt  --make-bed --out fine_loucs --keep-allele-order

for sum_file in `cat q`
do
zcat $sum_file |awk 'BEGIN{OFS=FS="\t"}{if($13<5e-8) print $0}' |awk 'BEGIN{OFS=FS="\t"}{print $1,$3-1500000,$3+1500000}' |awk 'BEGIN{OFS=FS="\t"}{print $1,$3-1500000,$3+1500000}' >q2_$sum_file &
done

for sum_file_bed in `cat q2`
do
bedtools merge -i $sum_file_bed >region_${sum_file_bed}.txt
done

bedtools merge -i   q2_229_PheCode.v1.0.fastGWA.gz
```
